{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 - Vectorizers\n",
    "## Apply of Glove & Word2Vec Embeddings on dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the data\n",
    "Dataset: \n",
    "https://raw.githubusercontent.com/subashgandyer/datasets/main/deepnlp_classification_data.zip"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2023-05-18 23:43:01--  https://raw.githubusercontent.com/subashgandyer/datasets/main/deepnlp_classification_data.zip\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8002::154, 2606:50c0:8003::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1373119 (1.3M) [application/zip]\n",
      "Saving to: 'deepnlp_classification_data.zip.2'\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  3% 1.66M 1s\n",
      "    50K .......... .......... .......... .......... ..........  7% 32.0M 0s\n",
      "   100K .......... .......... .......... .......... .......... 11% 1.82M 0s\n",
      "   150K .......... .......... .......... .......... .......... 14% 5.44M 0s\n",
      "   200K .......... .......... .......... .......... .......... 18% 2.71M 0s\n",
      "   250K .......... .......... .......... .......... .......... 22% 34.8M 0s\n",
      "   300K .......... .......... .......... .......... .......... 26% 1.53M 0s\n",
      "   350K .......... .......... .......... .......... .......... 29%  103M 0s\n",
      "   400K .......... .......... .......... .......... .......... 33% 3.81M 0s\n",
      "   450K .......... .......... .......... .......... .......... 37% 7.84M 0s\n",
      "   500K .......... .......... .......... .......... .......... 41% 6.29M 0s\n",
      "   550K .......... .......... .......... .......... .......... 44%  551K 0s\n",
      "   600K .......... .......... .......... .......... .......... 48% 17.9M 0s\n",
      "   650K .......... .......... .......... .......... .......... 52% 6.70M 0s\n",
      "   700K .......... .......... .......... .......... .......... 55% 8.03M 0s\n",
      "   750K .......... .......... .......... .......... .......... 59% 32.0M 0s\n",
      "   800K .......... .......... .......... .......... .......... 63% 9.74M 0s\n",
      "   850K .......... .......... .......... .......... .......... 67% 17.2M 0s\n",
      "   900K .......... .......... .......... .......... .......... 70% 28.4M 0s\n",
      "   950K .......... .......... .......... .......... .......... 74% 7.24M 0s\n",
      "  1000K .......... .......... .......... .......... .......... 78% 8.83M 0s\n",
      "  1050K .......... .......... .......... .......... .......... 82% 12.4M 0s\n",
      "  1100K .......... .......... .......... .......... .......... 85% 23.7M 0s\n",
      "  1150K .......... .......... .......... .......... .......... 89% 7.42M 0s\n",
      "  1200K .......... .......... .......... .......... .......... 93% 5.32M 0s\n",
      "  1250K .......... .......... .......... .......... .......... 96% 32.6M 0s\n",
      "  1300K .......... .......... .......... ..........           100% 16.4M=0.3s\n",
      "\n",
      "2023-05-18 23:43:01 (4.35 MB/s) - 'deepnlp_classification_data.zip.2' saved [1373119/1373119]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://raw.githubusercontent.com/subashgandyer/datasets/main/deepnlp_classification_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "zip_file_path = \"deepnlp_classification_data.zip\"\n",
    "\n",
    "# Open the zip file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    # Extract all the contents of the zip file\n",
    "    zip_ref.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"r8-train-all-terms.txt\"\n",
    "with open(train_file, \"r\") as train_f:\n",
    "    train_data = train_f.readlines()\n",
    "\n",
    "test_file = \"r8-test-all-terms.txt\"\n",
    "with open(test_file, \"r\") as test_f:\n",
    "    test_data = test_f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv('r8-train-all-terms.txt',header=None,sep='\\t')\n",
    "test = pd.read_csv('r8-test-all-terms.txt',header=None,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>earn</td>\n",
       "      <td>champion products ch approves stock split cham...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>acq</td>\n",
       "      <td>computer terminal systems cpml completes sale ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>earn</td>\n",
       "      <td>cobanco inc cbco year net shr cts vs dlrs net ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>earn</td>\n",
       "      <td>am international inc am nd qtr jan oper shr lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>earn</td>\n",
       "      <td>brown forman inc bfd th qtr net shr one dlr vs...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0                                                  1\n",
       "0  earn  champion products ch approves stock split cham...\n",
       "1   acq  computer terminal systems cpml completes sale ...\n",
       "2  earn  cobanco inc cbco year net shr cts vs dlrs net ...\n",
       "3  earn  am international inc am nd qtr jan oper shr lo...\n",
       "4  earn  brown forman inc bfd th qtr net shr one dlr vs..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trade</td>\n",
       "      <td>asian exporters fear damage from u s japan rif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>grain</td>\n",
       "      <td>china daily says vermin eat pct grain stocks a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ship</td>\n",
       "      <td>australian foreign ship ban ends but nsw ports...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>acq</td>\n",
       "      <td>sumitomo bank aims at quick recovery from merg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>earn</td>\n",
       "      <td>amatil proposes two for five bonus share issue...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0                                                  1\n",
       "0  trade  asian exporters fear damage from u s japan rif...\n",
       "1  grain  china daily says vermin eat pct grain stocks a...\n",
       "2   ship  australian foreign ship ban ends but nsw ports...\n",
       "3    acq  sumitomo bank aims at quick recovery from merg...\n",
       "4   earn  amatil proposes two for five bonus share issue..."
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns = ['word','related']\n",
    "test.columns = ['word','related']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Glove Vectorizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a GloveVectorizer Class\n",
    "- __init__\n",
    "- transform\n",
    "- fit_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloveVectorizer:\n",
    "  def __init__(self):\n",
    "    # load in pre-trained word vectors\n",
    "    print('Loading word vectors from Glove...')\n",
    "    word2vec = {}\n",
    "    embedding = []\n",
    "    idx2word = []\n",
    "    with open('glove.6B.50d.txt') as f:\n",
    "      # is just a space-separated text file in the format:\n",
    "      # word vec[0] vec[1] vec[2] ...\n",
    "      for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vec = np.asarray(values[1:], dtype='float32')\n",
    "        word2vec[word] = vec\n",
    "        embedding.append(vec)\n",
    "        idx2word.append(word)\n",
    "    print('Found %s word vectors.' % len(word2vec))\n",
    "\n",
    "    # save for later\n",
    "    self.word2vec = word2vec\n",
    "    self.embedding = np.array(embedding)\n",
    "    self.word2idx = {v:k for k,v in enumerate(idx2word)}\n",
    "    self.V, self.D = self.embedding.shape\n",
    "\n",
    "  def fit(self, data):\n",
    "    pass\n",
    "\n",
    "  def transform(self, data):\n",
    "    X = np.zeros((len(data), self.D))\n",
    "    n = 0\n",
    "    emptycount = 0\n",
    "    for sentence in data:\n",
    "      tokens = sentence.lower().split()\n",
    "      vecs = []\n",
    "      for word in tokens:\n",
    "        if word in self.word2vec:\n",
    "          vec = self.word2vec[word]\n",
    "          vecs.append(vec)\n",
    "      if len(vecs) > 0:\n",
    "        vecs = np.array(vecs)\n",
    "        X[n] = vecs.mean(axis=0)\n",
    "      else:\n",
    "        emptycount += 1\n",
    "      n += 1\n",
    "    print(\"Numer of samples with no words found: %s / %s\" % (emptycount, len(data)))\n",
    "    return X\n",
    "\n",
    "  def fit_transform(self, data):\n",
    "    self.fit(data)\n",
    "    return self.transform(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Glove Vectorizer object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word vectors from Glove...\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "glove=GloveVectorizer()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply vectorization on Training and Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>related</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>earn</td>\n",
       "      <td>champion products ch approves stock split cham...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>acq</td>\n",
       "      <td>computer terminal systems cpml completes sale ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>earn</td>\n",
       "      <td>cobanco inc cbco year net shr cts vs dlrs net ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>earn</td>\n",
       "      <td>am international inc am nd qtr jan oper shr lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>earn</td>\n",
       "      <td>brown forman inc bfd th qtr net shr one dlr vs...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word                                            related\n",
       "0  earn  champion products ch approves stock split cham...\n",
       "1   acq  computer terminal systems cpml completes sale ...\n",
       "2  earn  cobanco inc cbco year net shr cts vs dlrs net ...\n",
       "3  earn  am international inc am nd qtr jan oper shr lo...\n",
       "4  earn  brown forman inc bfd th qtr net shr one dlr vs..."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numer of samples with no words found: 0 / 5485\n",
      "Numer of samples with no words found: 0 / 2189\n"
     ]
    }
   ],
   "source": [
    "Xtrain = glove.fit_transform(train.related)\n",
    "Ytrain = train.word\n",
    "\n",
    "Xtest = glove.transform(test.related)\n",
    "Ytest = test.word"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model, train it, print scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9992707383773929"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf=RandomForestClassifier()\n",
    "rf.fit(Xtrain,Ytrain)\n",
    "\n",
    "rf.score(Xtrain,Ytrain)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9314755596162632"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(Xtest,Ytest)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Word2Vec Vectorizer\n",
    "\n",
    "### Google News Vector Model\n",
    "https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Word2VecVectorizer Class\n",
    "- __init__\n",
    "- transform\n",
    "- fit_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecVectorizer:\n",
    "  def __init__(self):\n",
    "    print(\"Loading word vectors from Word2Vec...\")\n",
    "    self.word_vectors = KeyedVectors.load_word2vec_format(\n",
    "      'GoogleNews-vectors-negative300.bin',\n",
    "      binary=True\n",
    "    )\n",
    "    print(\"Finished loading in word vectors\")\n",
    "\n",
    "  def fit(self, data):\n",
    "    pass\n",
    "\n",
    "  def transform(self, data):\n",
    "    # determine the dimensionality of vectors\n",
    "    v = self.word_vectors.get_vector('king')\n",
    "    self.D = v.shape[0]\n",
    "\n",
    "    X = np.zeros((len(data), self.D))\n",
    "    n = 0\n",
    "    emptycount = 0\n",
    "    for sentence in data:\n",
    "      tokens = sentence.split()\n",
    "      vecs = []\n",
    "      m = 0\n",
    "      for word in tokens:\n",
    "        try:\n",
    "          # throws KeyError if word not found\n",
    "          vec = self.word_vectors.get_vector(word)\n",
    "          vecs.append(vec)\n",
    "          m += 1\n",
    "        except KeyError:\n",
    "          pass\n",
    "      if len(vecs) > 0:\n",
    "        vecs = np.array(vecs)\n",
    "        X[n] = vecs.mean(axis=0)\n",
    "      else:\n",
    "        emptycount += 1\n",
    "      n += 1\n",
    "    print(\"Numer of samples with no words found: %s / %s\" % (emptycount, len(data)))\n",
    "    return X\n",
    "\n",
    "\n",
    "  def fit_transform(self, data):\n",
    "    self.fit(data)\n",
    "    return self.transform(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Word2Vec Vectorizer object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word vectors from Word2Vec...\n",
      "Finished loading in word vectors\n"
     ]
    }
   ],
   "source": [
    "w2v=Word2VecVectorizer()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply vectorization of training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numer of samples with no words found: 0 / 5485\n",
      "Numer of samples with no words found: 0 / 2189\n"
     ]
    }
   ],
   "source": [
    "xtrain = w2v.fit_transform(train.related)\n",
    "ytrain = train.word\n",
    "\n",
    "xtest = w2v.transform(test.related)\n",
    "ytest = test.word"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9992707383773929"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfinw2v=RandomForestClassifier()\n",
    "rfinw2v.fit(xtrain,ytrain)\n",
    "\n",
    "rfinw2v.score(xtrain,ytrain)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9387848332571951"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfinw2v.score(xtest,ytest)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights\n",
    "- Which Vectorizer is better for this dataset with RandomForest as Algorithm?\n",
    "- Which is the best Vectorizer when trying out different algorithms like SVM, NB, Logistic Regression?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Word2Vec Vectorizer with GoogleNews-vectors-negative300 model for me is better for this dataset with randomforest algorithm. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9396984924623115"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svcglove=SVC(kernel='rbf', class_weight='balanced')\n",
    "svcglove.fit(Xtrain,Ytrain)\n",
    "\n",
    "svcglove.score(Xtest,Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9328460484239379"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knnglove = KNeighborsClassifier()\n",
    "knnglove.fit(Xtrain,Ytrain)\n",
    "\n",
    "knnglove.score(Xtest,Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8720877112836912"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbglove = GaussianNB()\n",
    "nbglove.fit(Xtrain,Ytrain)\n",
    "\n",
    "nbglove.score(Xtest,Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9671082686158063"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svcw2v=SVC(kernel='rbf', class_weight='balanced')\n",
    "svcw2v.fit(xtrain,ytrain)\n",
    "\n",
    "svcw2v.score(xtest,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94792142530836"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knnw2v = KNeighborsClassifier()\n",
    "knnw2v.fit(xtrain,ytrain)\n",
    "\n",
    "knnw2v.score(xtest,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8547281863864779"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbw2v = GaussianNB()\n",
    "nbw2v.fit(xtrain,ytrain)\n",
    "\n",
    "nbw2v.score(xtest,ytest)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried Word2vec vectorizer and Glove vectorizer, with SVM, Gaussian Naive Bayes, and KNN algorithms. \n",
    "\n",
    "Word2vec with SVM performs the best for me, and Word2vec with Gaussian Naive Bayes performs the worst for me. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-Fangyi",
   "language": "python",
   "name": "ml-fangyi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
